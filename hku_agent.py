#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HKU AIå¯¹è¯Agent - å®Œæ•´ç‰ˆ
ç‰¹ç‚¹ï¼š
1. æ”¯æŒäººè®¾åˆ‡æ¢
2. æŸ¥è¯¢æ‰©å±•
3. å¤šè½®æ£€ç´¢
4. å¯¹è¯è®°å¿†
"""

import json
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import re
import requests
from typing import List, Dict, Optional
from dataclasses import dataclass
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer


# ==================== é…ç½® ====================
class Config:
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
    DEEPSEEK_URL = "https://api.deepseek.com/chat/completions" # æ³¨æ„ URL ä¿®æ­£
    
    # ã€ä¿®æ”¹ç‚¹ã€‘ä¸å†ä½¿ç”¨ API embeddingï¼Œæ”¹ç”¨æœ¬åœ°è½»é‡çº§æ¨¡å‹
    USE_LOCAL_EMBEDDING = True
    LOCAL_EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

    KNOWLEDGE_BASE_DIR = "knowledge_base"
    TOP_K = 10
    HYBRID_ALPHA = 0.3 # ç¨å¾®é™ä½å‘é‡æƒé‡ï¼Œé˜²æ­¢å› æ¨¡å‹å·®å¼‚å¯¼è‡´æ£€ç´¢åç¦»

# ==================== ç®€å•æ£€ç´¢å™¨ ====================
class SimpleRetriever:
    """åŸºäºå…³é”®è¯çš„æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.documents = []
    
    def add_document(self, content: str, source: str, metadata: dict = None):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents.append({
            'content': content,
            'source': source,
            'metadata': metadata or {}
        })
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.documents:
            return []
        
        query_keywords = self._extract_keywords(query)
        
        scored_docs = []
        for doc in self.documents:
            score = self._calculate_score(query_keywords, doc['content'])
            if score > 0:
                scored_docs.append((score, doc))
        
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored_docs[:top_k]]
    
    def _extract_keywords(self, text: str) -> set:
        """æå–å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰"""
        keywords = set()
        
        # è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-zA-Z]+', text.lower())
        keywords.update(w for w in english_words if len(w) > 2)
        
        # ä¸­æ–‡2-4å­—è¯ç»„
        chinese_chars = [c for c in text if '\u4e00' <= c <= '\u9fff']
        chinese_text = ''.join(chinese_chars)
        
        for length in [2, 3, 4]:
            for i in range(len(chinese_text) - length + 1):
                keywords.add(chinese_text[i:i+length])
        
        # æ•°å­—
        numbers = re.findall(r'\d+', text)
        keywords.update(numbers)
        
        return keywords
    
    def _calculate_score(self, query_keywords: set, document: str) -> float:
        """è®¡ç®—æ–‡æ¡£å¾—åˆ†"""
        doc_lower = document.lower()
        doc_keywords = self._extract_keywords(document)
        
        score = 0.0
        
        matched = query_keywords & doc_keywords
        if query_keywords:
            score += len(matched) / len(query_keywords) * 2.0
        
        for keyword in query_keywords:
            if len(keyword) > 1 and keyword in doc_lower:
                score += 0.5
        
        synonyms = {
            'hku': ['é¦™æ¸¯å¤§å­¦', 'æ¸¯å¤§'],
            'é¦™æ¸¯å¤§å­¦': ['hku', 'æ¸¯å¤§'],
            'æ¸¯å¤§': ['hku', 'é¦™æ¸¯å¤§å­¦'],
            'å­¦é™¢': ['faculty', 'é™¢ç³»'],
            'æ’å': ['rank', 'qs', 'æ³°æ™¤å£«'],
            'æˆç«‹': ['å»ºç«‹', 'åˆ›åŠ', '1911'],
        }
        
        for keyword in query_keywords:
            if keyword in synonyms:
                for syn in synonyms[keyword]:
                    if syn in doc_lower:
                        score += 0.3
        
        return score

# ==================== å‘é‡æ£€ç´¢å™¨ ====================

class VectorRetriever:
    """åŸºäºæœ¬åœ°æ¨¡å‹çš„å‘é‡æ£€ç´¢ï¼ˆç¨³å®šã€å…è´¹ï¼‰"""
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None

    def add_document(self, content: str, source: str, metadata: dict = None):
        self.documents.append({
            "content": content,
            "source": source,
            "metadata": metadata or {}
        })

    def build_index(self):
        if not self.documents:
            return
        print("â³ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼• (æœ¬åœ°æ¨¡å‹)...")
        texts = [d["content"] for d in self.documents]
        # encode ç›´æ¥è¿”å› numpy array
        self.embeddings = self.model.encode(texts, normalize_embeddings=True)
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    def search(self, query: str, top_k: int = 5):
        if self.embeddings is None or not self.documents:
            return []
        
        q_emb = self.model.encode([query], normalize_embeddings=True)
        # è®¡ç®—ç›¸ä¼¼åº¦
        sims = cosine_similarity(q_emb, self.embeddings)[0]
        
        # è·å– top_k
        idxs = np.argsort(-sims)[:top_k]
        return [self.documents[i] | {"score": float(sims[i])} for i in idxs]


# ==================== Hybrid æ£€ç´¢å™¨ ====================
class HybridRetriever:
    """å‘é‡ + å…³é”®è¯ æ··åˆæ£€ç´¢"""
    def __init__(self, keyword_ret: SimpleRetriever, vector_ret: VectorRetriever,
                 alpha: float = 0.6):
        self.keyword_ret = keyword_ret
        self.vector_ret = vector_ret
        self.alpha = alpha

    def search(self, query: str, top_k: int = 5,
               keyword_top_k: int = 8, vector_top_k: int = 8):
        kw_docs = self.keyword_ret.search(query, top_k=keyword_top_k)
        vec_docs = self.vector_ret.search(query, top_k=vector_top_k)

        # ç”¨ content åšå»é‡å¹¶æ‰“åˆ†èåˆ
        merged = {}
        for rank, d in enumerate(kw_docs):
            key = d["content"]
            # å…³é”®è¯å¾—åˆ†ç”¨ rank è¿‘ä¼¼ï¼šè¶Šé å‰è¶Šé«˜
            kw_score = 1.0 / (rank + 1)
            merged[key] = (d, kw_score, 0.0)

        for rank, d in enumerate(vec_docs):
            key = d["content"]
            vec_score = d.get("score", 1.0 / (rank + 1))
            if key in merged:
                doc, kw_score, _ = merged[key]
                merged[key] = (doc, kw_score, vec_score)
            else:
                merged[key] = (d, 0.0, vec_score)

        scored = []
        for doc, kw_s, vec_s in merged.values():
            final_s = (1 - self.alpha) * kw_s + self.alpha * vec_s
            scored.append((final_s, doc))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [doc for _, doc in scored[:top_k]]


# ==================== çŸ¥è¯†åº“ç®¡ç† ====================
class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, kb_dir: str):
        self.kb_dir = kb_dir

        # ä¸¤å¥—æ£€ç´¢å™¨
        self.keyword_retriever = SimpleRetriever()

        self.vector_retriever = VectorRetriever(Config.LOCAL_EMBED_MODEL)


        self.retriever = HybridRetriever(
            self.keyword_retriever,
            self.vector_retriever,
            alpha=Config.HYBRID_ALPHA
        )

        os.makedirs(kb_dir, exist_ok=True)


    
    def load(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        print(f"\nğŸ“š åŠ è½½çŸ¥è¯†åº“: {self.kb_dir}")
        
        if not os.listdir(self.kb_dir):
            self._create_samples()
        
        files = [f for f in os.listdir(self.kb_dir) 
                if f.endswith(('.txt', '.md'))]
        
        total_docs = 0
        for filename in files:
            filepath = os.path.join(self.kb_dir, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            paragraphs = [p.strip() for p in content.split('\n\n') 
                         if p.strip() and len(p.strip()) > 2]
            
            for para in paragraphs:
                self.keyword_retriever.add_document(
                    content=para,
                    source=filename,
                    metadata={'length': len(para)}
                )
                self.vector_retriever.add_document(
                    content=para,
                    source=filename,
                    metadata={'length': len(para)}
                )
                total_docs += 1

            
            print(f"  âœ“ {filename}: {len(paragraphs)} æ®µ")
        # å»ºç«‹å‘é‡ç´¢å¼•
        try:
            self.vector_retriever.build_index()
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥ï¼Œè‡ªåŠ¨é™çº§ä¸ºå…³é”®è¯æ£€ç´¢ã€‚é”™è¯¯: {e}")
            self.retriever = self.keyword_retriever


        print(f"âœ… å…±åŠ è½½ {total_docs} ä¸ªæ–‡æ¡£ç‰‡æ®µ\n")
    
    def _create_samples(self):
        """åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“"""
        samples = {
            "hku_basic.txt": """é¦™æ¸¯å¤§å­¦ï¼ˆThe University of Hong Kongï¼Œç®€ç§°HKUæˆ–æ¸¯å¤§ï¼‰æ˜¯é¦™æ¸¯å†å²æœ€æ‚ ä¹…çš„é«˜ç­‰æ•™è‚²æœºæ„ã€‚

æˆç«‹æ—¶é—´ï¼š1911å¹´3æœˆ30æ—¥
åœ°ç†ä½ç½®ï¼šé¦™æ¸¯å²›è–„æ‰¶æ—é“
æ ¡è®­ï¼šæ˜å¾·æ ¼ç‰©ï¼ˆSapientia Et Virtusï¼‰
å­¦æ ¡æ€§è´¨ï¼šå…¬ç«‹ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦""",

            "hku_faculties.txt": """é¦™æ¸¯å¤§å­¦è®¾æœ‰åå¤§å­¦é™¢ï¼š

1. å»ºç­‘å­¦é™¢ (Faculty of Architecture)
2. æ–‡å­¦é™¢ (Faculty of Arts)
3. ç»æµåŠå·¥å•†ç®¡ç†å­¦é™¢ (Faculty of Business and Economics)
4. ç‰™åŒ»å­¦é™¢ (Faculty of Dentistry)
5. æ•™è‚²å­¦é™¢ (Faculty of Education)
6. å·¥ç¨‹å­¦é™¢ (Faculty of Engineering)
7. æ³•å¾‹å­¦é™¢ (Faculty of Law)
8. æå˜‰è¯šåŒ»å­¦é™¢ (Li Ka Shing Faculty of Medicine)
9. ç†å­¦é™¢ (Faculty of Science)
10. ç¤¾ä¼šç§‘å­¦å­¦é™¢ (Faculty of Social Sciences)""",

            "hku_rankings.txt": """é¦™æ¸¯å¤§å­¦ä¸–ç•Œæ’åï¼š

QS 2024ï¼šå…¨çƒç¬¬26ä½
THE 2024ï¼šå…¨çƒç¬¬35ä½

å­¦ç§‘ä¼˜åŠ¿ï¼š
- ç‰™åŒ»å­¦ï¼šå…¨çƒç¬¬4ä½
- æ•™è‚²å­¦ï¼šå…¨çƒç¬¬7ä½
- å»ºç­‘å­¦ï¼šå…¨çƒç¬¬14ä½"""
        }
        
        for filename, content in samples.items():
            filepath = os.path.join(self.kb_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)

# ==================== LLMå®¢æˆ·ç«¯ ====================
class LLMClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        # ç¡®è®¤ URL æ˜¯å¯¹çš„
        self.api_url = "https://api.deepseek.com/chat/completions"
    
    def call(self, messages: List[Dict], temperature: float = 0.3, 
             max_tokens: int = 1000) -> Optional[str]:
        """è°ƒç”¨API"""
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        try:
            # ã€ä¿®æ”¹ç‚¹1ã€‘æŠŠè¶…æ—¶æ—¶é—´ä» 30 æ”¹æˆ 60 ç§’
            # å¤æ‚çš„äººè®¾ç”Ÿæˆæ¯”è¾ƒæ…¢ï¼Œ30ç§’ç»å¸¸ä¸å¤Ÿ
            response = requests.post(
                self.api_url,
                headers=headers,
                json=data,
                timeout=120
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content']
            else:
                # ã€ä¿®æ”¹ç‚¹2ã€‘æ‰“å°å‡ºå…·ä½“çš„é”™è¯¯åŸå› ï¼Œä¸è¦åªè¿”å› None
                print(f"\nâŒ [API Error] çŠ¶æ€ç : {response.status_code}")
                print(f"âŒ [API Error] è¯¦æƒ…: {response.text}")
                return None
                
        except Exception as e:
            # ã€ä¿®æ”¹ç‚¹3ã€‘æ‰“å°å‡ºç½‘ç»œé”™è¯¯è¯¦æƒ…
            print(f"\nâŒ [Network Error] è¿æ¥æŠ¥é”™: {e}")
            return None

# ==================== HKU Agent ====================
class HKUAgent:
    """HKU AIå¯¹è¯Agent"""
    
    def __init__(self, kb: KnowledgeBase, llm: LLMClient):
        self.kb = kb
        self.llm = llm
        self.conversation_history = []
        self.persona = None  # å½“å‰äººè®¾
    
    def chat(self, user_query: str) -> str:
        """æ ‡å‡†å¯¹è¯æµç¨‹"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_query}")
        if self.persona:
            print(f"ğŸ­ äººè®¾: {self.persona.get('name', 'é»˜è®¤')}")
        print(f"{'='*60}\n")
        
        # 1. æŸ¥è¯¢æ‰©å±•
        print("ğŸ§  åˆ†ææŸ¥è¯¢...")
        expanded_queries = self._expand_query(user_query)
        print(f"âœ“ ç”Ÿæˆ {len(expanded_queries)} ä¸ªæ£€ç´¢æŸ¥è¯¢\n")
        
        # 2. å¤šè½®æ£€ç´¢
        print("ğŸ” æ‰§è¡Œæ£€ç´¢...")
        all_docs = []
        seen_content = set()
        
        for query in expanded_queries:
            docs = self.kb.retriever.search(query, top_k=3)
            for doc in docs:
                content_hash = hash(doc['content'][:100])
                if content_hash not in seen_content:
                    all_docs.append(doc)
                    seen_content.add(content_hash)
        
        print(f"âœ“ æ‰¾åˆ° {len(all_docs)} ä¸ªç‰‡æ®µ\n")
        
        # 3. é‡æ’åº
        if len(all_docs) > Config.TOP_K:
            all_docs = self._rerank_documents(user_query, all_docs)[:Config.TOP_K]
        
        # 4. ç”Ÿæˆç­”æ¡ˆ
        print("ğŸ¤– AIç”Ÿæˆå›ç­”...\n")
        answer = self._generate_answer(user_query, all_docs)
        
        # 5. è®°å½•å†å²
        self.conversation_history.append({
            'user': user_query,
            'assistant': answer,
            'sources': [doc['source'] for doc in all_docs]
        })
        
        return answer
    
    def _expand_query(self, query: str) -> List[str]:
        """æŸ¥è¯¢æ‰©å±•"""
        
        prompt = f"""ä½ æ˜¯æ£€ç´¢ä¸“å®¶ã€‚ç”¨æˆ·é—®é¢˜æ˜¯ï¼š"{query}"

ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æ£€ç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ï¼š"""

        messages = [{"role": "user", "content": prompt}]
        response = self.llm.call(messages, temperature=0.3, max_tokens=200)
        
        if response:
            queries = [q.strip() for q in response.split('\n') if q.strip()]
            if query not in queries:
                queries.insert(0, query)
            return queries[:5]
        else:
            return [query]
    
    def _rerank_documents(self, query: str, docs: List[Dict]) -> List[Dict]:
        """æ–‡æ¡£é‡æ’åº"""
        
        query_keywords = set(self.kb.keyword_retriever._extract_keywords(query))
        
        scored = []
        for doc in docs:
            doc_keywords = set(self.kb.keyword_retriever._extract_keywords(doc['content']))
            overlap = len(query_keywords & doc_keywords)
            relevance = overlap / len(query_keywords) if query_keywords else 0
            scored.append((relevance, doc))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [doc for _, doc in scored]
    

    def _generate_answer(self, query: str, docs: List[Dict]) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆæé€Ÿç‰ˆï¼šæ”¾å¼ƒ JSONï¼Œä½¿ç”¨çº¯æ–‡æœ¬è§£æï¼Œå¤§å¹…æå‡é€Ÿåº¦ï¼‰
        """
        # 1. å…œåº•
        if not docs:
            return "å¾ˆæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰ç›¸å…³èµ„æ–™ã€‚å»ºè®®ç›´æ¥è®¿é—® HKU å®˜ç½‘ (www.hku.hk) æŸ¥è¯¢ã€‚"

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_blocks = []
        for i, doc in enumerate(docs):
            if len(doc['content']) > 5:
                context_blocks.append(f"[èµ„æ–™{i+1}] {doc['content']}")
        context_str = "\n\n".join(context_blocks)

        # 3. æ„å»º System Prompt (ç§»é™¤å¤æ‚çš„ JSON è¦æ±‚)
        base_prompt = self.persona['prompt'] if self.persona else "ä½ æ˜¯HKU AIåŠ©æ‰‹ã€‚"
        
        system_prompt = f"""{base_prompt}

----------------
ã€å›ç­”è§„åˆ™ã€‘
1. **äº‹å®å¼•ç”¨**ï¼šæ¶‰åŠå®¢è§‚äº‹å®ï¼ˆæ•°æ®ã€æ”¿ç­–ï¼‰ï¼Œå¿…é¡»åœ¨å¥å°¾æ ‡æ³¨æ¥æºï¼Œå¦‚[1][2]ã€‚
2. **äººè®¾å‘æŒ¥**ï¼šé’ˆå¯¹ç”¨æˆ·çš„æƒ…æ„Ÿæˆ–å’¨è¯¢ï¼Œè¯·å¤§èƒ†å‘æŒ¥äººè®¾ï¼ˆå­¦é•¿/å¯¼å¸ˆï¼‰è¿›è¡Œäº¤æµï¼Œè¿™éƒ¨åˆ†**ä¸éœ€è¦å¼•ç”¨**ã€‚
3. **æ ¼å¼è¦æ±‚**ï¼šè¯·ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ï¼Œä¸è¦ä»»ä½• JSON æ ¼å¼ï¼Œä¹Ÿä¸è¦ Markdown ä»£ç å—ã€‚
"""

        user_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nå‚è€ƒèµ„æ–™ï¼š\n{context_str}"

        # 4. è°ƒç”¨ LLM (ç§»é™¤ JSON å‹åŠ›ï¼Œé€Ÿåº¦ä¼šå¿«å¾ˆå¤š)
        # max_tokens é™åˆ¶åœ¨ 800ï¼Œé˜²æ­¢åºŸè¯å¤ªå¤šå¯¼è‡´è¶…æ—¶
        raw_response = self.llm.call([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], temperature=0.4, max_tokens=800)

        if not raw_response:
            print("âš ï¸ [é”™è¯¯] LLM æ¥å£æ— å“åº”")
            return "æŠ±æ­‰ï¼ŒAI æ­¤æ—¶æœ‰ç‚¹ç¹å¿™ï¼Œè¯·å†è¯•ä¸€æ¬¡ã€‚"

        answer = raw_response.strip()

        # 5. åç«¯è‡ªåŠ¨æå–å¼•ç”¨ (æ›¿ä»£ JSON è§£æ)
        # ä½¿ç”¨æ­£åˆ™ä»æ–‡æœ¬ä¸­æå– [1], [2] è¿™æ ·çš„ç¼–å·
        import re
        found_citations = re.findall(r'\[(\d+)\]', answer)
        # å»é‡å¹¶è½¬ä¸ºæ•´æ•°
        citations = sorted(list(set([int(c) for c in found_citations])))
        
        # é»˜è®¤ç½®ä¿¡åº¦ (çº¯æ–‡æœ¬æ¨¡å¼ä¸‹æ¨¡å‹ä¸è¾“å‡ºç½®ä¿¡åº¦ï¼Œæˆ‘ä»¬æ ¹æ®æ˜¯å¦æœ‰å¼•ç”¨æ¥ç»™åˆ†)
        confidence = 0.85 if citations else 0.5

        # 6. å¼•ç”¨æ¸…æ´—
        valid_ids = set(range(1, len(docs) + 1))
        safe_citations = [c for c in citations if c in valid_ids]
        
        # 7. Verifier æ ¡éªŒ (ä¾ç„¶ä¿ç•™ï¼Œä½†é€»è¾‘ä¸å˜)
        is_valid = self._verify_answer(query, answer, safe_citations, docs)
        
        if not is_valid:
            print("âš ï¸ [Verifier] æ ¡éªŒä¸é€šè¿‡ï¼Œå°è¯•ä¿®æ­£...")
            retry_prompt = f"""ä½ åˆšæ‰çš„å›ç­”ä¸­ï¼Œå®¢è§‚æ•°æ®å¯èƒ½æœ‰è¯¯ã€‚
è¯·ä¿æŒã€äººè®¾è¯­æ°”ã€‘ï¼Œä½†ä¿®æ­£ã€å®¢è§‚äº‹å®ã€‘ï¼Œç¡®ä¿æ•°æ®æºäºä»¥ä¸‹èµ„æ–™ã€‚

èµ„æ–™ï¼š
{context_str}

é—®é¢˜ï¼š{query}"""
            
            retry_ans = self.llm.call([{"role": "user", "content": retry_prompt}], temperature=0.3)
            if retry_ans:
                answer = retry_ans
                confidence = 0.6
            else:
                return "æŠ±æ­‰ï¼Œæˆ‘å¤ªæƒ³å¸®ä½ äº†ï¼Œä½†èµ„æ–™ä¸è¶³ä»¥æ”¯æŒå‡†ç¡®å»ºè®®ã€‚"

        # 8. æœ€ç»ˆè¾“å‡º
        cited_sources = sorted(list(set([docs[i-1]['source'] for i in safe_citations])))
        src_text = " | ".join(cited_sources) if cited_sources else "HKUçŸ¥è¯†åº“"
        
        return f"{answer}\n\nğŸ“Š ç½®ä¿¡åº¦: {confidence:.2f}\nğŸ“š æ¥æº: {src_text}"


    def _verify_answer(self, query: str, answer: str, citations: List[int], docs: List[Dict]) -> bool:
        """
        éªŒè¯å™¨ï¼šä¸¥æ ¼åŒºåˆ†ã€ç¡¬æ•°æ®ã€‘å’Œã€è½¯æƒ…æ„Ÿã€‘
        """
        # æå–è¯æ®
        evidence = []
        for c in citations:
            if 0 < c <= len(docs):
                evidence.append(docs[c-1]['content'])
        evidence_text = "\n".join(evidence)

        # å¦‚æœæ²¡æœ‰å¼•ç”¨ä½†å›ç­”å¾ˆé•¿ï¼Œä¸”åŒ…å«æƒ…æ„Ÿè¯ï¼Œæ”¾è¡Œ
        if not evidence_text:
            if len(answer) > 20: 
                return True
            return False

        verifier_prompt = f"""ä»»åŠ¡ï¼šäº‹å®æ ¸æŸ¥ã€‚

ã€åŸåˆ™ã€‘
1. åªæ ¸æŸ¥ã€å®¢è§‚äº‹å®ã€‘ï¼ˆæ•°å­—ã€æ—¶é—´ã€åœ°ç‚¹ã€äººåã€æ”¿ç­–ï¼‰ã€‚
2. å¿½ç•¥ã€ä¸»è§‚å†…å®¹ã€‘ï¼ˆå®‰æ…°ã€å»ºè®®ã€é¼“åŠ±ã€äººè®¾è¯­æ°”ï¼‰ã€‚ä¸»è§‚å†…å®¹ä¸éœ€è¦è¯æ®ã€‚

é—®é¢˜ï¼š{query}
å›ç­”ï¼š{answer}

è¯æ®ï¼š
{evidence_text}

è¯·åˆ¤æ–­ï¼š
å›ç­”ä¸­çš„ã€å®¢è§‚äº‹å®ã€‘æ˜¯å¦ä¸è¯æ®çŸ›ç›¾ï¼Ÿ
- å¦‚æœåªæ˜¯åŠ äº†å¥"åˆ«æ‹…å¿ƒ"ï¼Œä½†æ•°æ®æ˜¯å¯¹çš„ -> è¾“å‡º YES
- å¦‚æœæ•°æ®é”™äº† -> è¾“å‡º NO
- å¦‚æœå…¨æ˜¯å®‰æ…°è¯ï¼Œæ²¡ææ•°æ® -> è¾“å‡º YES

åªè¾“å‡º YES æˆ– NOã€‚"""

        # è°ƒç”¨ LLM
        res = self.llm.call([{"role": "user", "content": verifier_prompt}], temperature=0.1, max_tokens=5)
        
        # ã€å…³é”®ä¿®å¤ã€‘åˆ¤ç©º
        if not res:
            # å¦‚æœ Verifier æŒ‚äº†ï¼Œé»˜è®¤æ”¾è¡Œï¼ˆå®å¯é”™æ€ä¸å¯ä¸ç­”ï¼‰
            return True
            
        if "NO" in res.upper():
            return False
        return True


    def _verify_answer(self, query: str, answer: str, citations: List[int], docs: List[Dict]) -> bool:
        """
        éªŒè¯å™¨ï¼šä¸¥æ ¼åŒºåˆ†ã€ç¡¬æ•°æ®ã€‘å’Œã€è½¯æƒ…æ„Ÿã€‘
        """
        # æå–è¯æ®æ–‡æœ¬
        evidence = []
        for c in citations:
            if 0 < c <= len(docs):
                evidence.append(docs[c-1]['content'])
        evidence_text = "\n".join(evidence)

        # å¦‚æœæ²¡æœ‰å¼•ç”¨ï¼Œä½†å›ç­”å¾ˆé•¿ï¼Œå¯èƒ½æ˜¯çº¯é—²èŠï¼Œæ”¾è¡Œ
        if not evidence_text and len(answer) > 10:
            return True

        verifier_prompt = f"""ä»»åŠ¡ï¼šäº‹å®æ ¸æŸ¥ã€‚

ã€åŸåˆ™ã€‘
1. æˆ‘ä»¬**åªæ ¸æŸ¥**å®¢è§‚äº‹å®ï¼ˆæ•°å­—ã€æ—¶é—´ã€åœ°ç‚¹ã€äººåã€æ”¿ç­–ï¼‰ã€‚
2. æˆ‘ä»¬**å®Œå…¨å¿½ç•¥**ä¸»è§‚å†…å®¹ï¼ˆå®‰æ…°ã€å»ºè®®ã€é¼“åŠ±ã€äººè®¾è¯­æ°”ï¼‰ã€‚ä¸»è§‚å†…å®¹ä¸éœ€è¦è¯æ®ã€‚

é—®é¢˜ï¼š{query}
å›ç­”ï¼š{answer}

è¯æ®ï¼š
{evidence_text}

è¯·åˆ¤æ–­ï¼š
å›ç­”ä¸­æåˆ°çš„ã€å®¢è§‚äº‹å®ã€‘æ˜¯å¦ä¸è¯æ®çŸ›ç›¾ï¼Œæˆ–å‡­ç©ºæé€ äº†è¯æ®ä¸­æ²¡æœ‰çš„ã€æ•°æ®ã€‘ï¼Ÿ
- å¦‚æœåªæ˜¯åŠ äº†å¥"åˆ«æ‹…å¿ƒ"ï¼Œä½†æ•°æ®æ˜¯å¯¹çš„ -> è¾“å‡º YES
- å¦‚æœæ•°æ®é”™äº† -> è¾“å‡º NO
- å¦‚æœå…¨æ˜¯å®‰æ…°è¯ï¼Œæ²¡ææ•°æ® -> è¾“å‡º YES

åªè¾“å‡º YES æˆ– NOã€‚"""

        # æ¸©åº¦è®¾ä¸º 0.1ï¼Œè®©å®ƒç¨å¾®çµæ´»ä¸€ç‚¹ç‚¹ï¼Œåˆ«å¤ªæ­»æ¿
        res = self.llm.call([{"role": "user", "content": verifier_prompt}], temperature=0.1, max_tokens=5)
        
        if res and "NO" in res.upper():
            return False
        return True


    def _safe_parse_json(self, text: str) -> Optional[dict]:
        """è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨çš„JSONè§£æ"""
        if not text: return None
        try:
            # 1. å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except:
            # 2. å°è¯•æå– Markdown ä»£ç å— ```json ... ```
            match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1))
                except:
                    pass
            # 3. å°è¯•æå–æœ€å¤–å±‚çš„ {}
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
            return None


    def _safe_parse_json(self, text: str) -> Optional[dict]:
        """å®¹é”™è§£æ JSON"""
        try:
            return json.loads(text)
        except Exception:
            # å°è¯•æˆªå–ç¬¬ä¸€ä¸ª {...}
            m = re.search(r'\{.*\}', text, re.S)
            if not m:
                return None
            try:
                return json.loads(m.group())
            except Exception:
                return None
# ==================== ä¸»ç¨‹åº ====================
def main():
    print("\nğŸ“ HKU AIå¯¹è¯Agent")
    print("="*60)
    
    kb = KnowledgeBase(Config.KNOWLEDGE_BASE_DIR)
    kb.load()
    
    llm = LLMClient(Config.DEEPSEEK_API_KEY)
    agent = HKUAgent(kb, llm)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            
            if not user_input:
                continue
            
            response = agent.chat(user_input)
            print(f"\nğŸ¤– åŠ©æ‰‹:\n{response}\n")
            print("="*60)
            
        except KeyboardInterrupt:
            print("\n\nå†è§!")
            break

if __name__ == "__main__":
    main()